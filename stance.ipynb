{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64154fc6-e3b7-49c7-8d1f-e4f780c8a970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1253dd1b-660d-4dc6-a83d-2c28fbd0aacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.dataset import DataSet\n",
    "from utils.generate_test_splits import generate_hold_out_split, read_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33c129e9-81ce-41c7-9b39-f32ddbd45702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset\n",
      "Total stances: 49972\n",
      "Total bodies: 1683\n"
     ]
    }
   ],
   "source": [
    "d = DataSet()\n",
    "generate_hold_out_split(d)\n",
    "trainID = set(read_ids(\"training_ids.txt\", \"splits\"))\n",
    "valID = set(read_ids(\"hold_out_ids.txt\", \"splits\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218d497e-a281-483e-919d-f7de19848b76",
   "metadata": {},
   "source": [
    "# Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddbb7929-7d1b-4d19-bb39-44616297ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_PER_ART = 20\n",
    "MAX_SENT_LEN = 30\n",
    "MAX_VOCAB = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde4bb2d-2708-47a7-973a-01e5fcc8a90d",
   "metadata": {},
   "source": [
    "# Set up training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b760b24-8e38-448d-907d-7af8f556427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stances = [stance for stance in d.stances if stance['Body ID'] in trainID]\n",
    "train_headlines = [stance['Headline'] for stance in train_stances]\n",
    "train_labels = [stance['Stance'] for stance in train_stances]\n",
    "train_body = [d.articles[stance['Body ID']] for stance in train_stances]\n",
    "\n",
    "val_stances = [stance for stance in d.stances if stance['Body ID'] in valID]\n",
    "val_headlines = [stance['Headline'] for stance in val_stances]\n",
    "val_labels = [stance['Stance'] for stance in val_stances]\n",
    "val_body = [d.articles[stance['Body ID']] for stance in val_stances]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65809091-a13b-4e7d-8a32-868df58d08fe",
   "metadata": {},
   "source": [
    "# Vectorization and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f78f7498-12c2-40fb-a2bb-81a0bcbca829",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sw26wong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "642b1bba-9a57-4a8f-a10a-f6ac30213805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB)\n",
    "tokenizer.fit_on_texts(train_body + train_headlines + val_body + val_headlines)\n",
    "\n",
    "from nltk import tokenize\n",
    "\n",
    "sent_tok_art = []\n",
    "for article in train_body:\n",
    "    sent_tok_art.append(tokenize.sent_tokenize(article))\n",
    "\n",
    "vsent_tok_art = []\n",
    "for article in val_body:\n",
    "    vsent_tok_art.append(tokenize.sent_tokenize(article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26f98757-8f2e-426d-8876-ffae9b238771",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_body = np.zeros((len(train_stances), MAX_SENT_PER_ART, MAX_SENT_LEN), dtype='int32')\n",
    "\n",
    "for i, article in enumerate(sent_tok_art):\n",
    "    for j, sentence in enumerate(article[:MAX_SENT_PER_ART]):\n",
    "        words = text_to_word_sequence(sentence)\n",
    "        for k, word in enumerate(words[:MAX_SENT_LEN]):\n",
    "            X_train_body[i][j][k] = tokenizer.word_index[word]\n",
    "\n",
    "X_train_head = np.zeros((len(train_stances), MAX_SENT_LEN), dtype='int32')\n",
    "\n",
    "for i, headline in enumerate(train_headlines):\n",
    "    words = text_to_word_sequence(headline)\n",
    "    for j, word in enumerate(words[:MAX_SENT_LEN]):\n",
    "        X_train_head[i][j] = tokenizer.word_index[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef3ec0ee-03cc-4a6f-8fd4-3a0b8dc472df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_body = np.zeros((len(val_stances), MAX_SENT_PER_ART, MAX_SENT_LEN), dtype='int32')\n",
    "\n",
    "for i, article in enumerate(vsent_tok_art):\n",
    "    for j, sentence in enumerate(article[:MAX_SENT_PER_ART]):\n",
    "        words = text_to_word_sequence(sentence)\n",
    "        for k, word in enumerate(words[:MAX_SENT_LEN]):\n",
    "            X_val_body[i][j][k] = tokenizer.word_index[word]\n",
    "\n",
    "X_val_head = np.zeros((len(val_stances), MAX_SENT_LEN), dtype='int32')\n",
    "\n",
    "for i, headline in enumerate(val_headlines):\n",
    "    words = text_to_word_sequence(headline)\n",
    "    for j, word in enumerate(words[:MAX_SENT_LEN]):\n",
    "        X_val_head[i][j] = tokenizer.word_index[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfd06aba-72ed-49c9-98b0-7bac194968ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = pd.Series(train_labels)\n",
    "one_hot = pd.get_dummies(targets,sparse = True)\n",
    "one_hot_labels = np.asarray(one_hot)\n",
    "y_train = one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68403e35-626a-42d6-aa04-526e60dabd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = pd.Series(val_labels)\n",
    "one_hot = pd.get_dummies(targets,sparse = True)\n",
    "one_hot_labels = np.asarray(one_hot)\n",
    "y_val = one_hot_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a94ed6b-68b8-4b6e-8474-7fffd9e9a84a",
   "metadata": {},
   "source": [
    "# Create Embedding Matrix from Google word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5116d1d-7460-4dfa-a13e-a4fd3c49fd37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27873"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3323e80-750a-4051-be75-c556b5707f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index)\n",
    "embedding_matrix = np.zeros((vocab_size+1, 300))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    try:\n",
    "        v = wv[word]\n",
    "        embedding_matrix[i] = v\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2260b21a-ab1e-4839-895d-220f5f01943d",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1b4925d-7332-4af8-b50e-9e91114af8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM, TimeDistributed, Activation\n",
    "from keras.layers import Flatten, Permute, merge, Input\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Input,Dense,multiply,concatenate,Dropout\n",
    "from keras.layers import GRU, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c6624ea-0440-4024-b8fd-12392d3683f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 300\n",
    "\n",
    "sentence_input = Input(shape=(MAX_SENT_LEN,),dtype='int32')\n",
    "\n",
    "embedded_sequences = Embedding(output_dim = hidden_size, input_dim = vocab_size+1, input_length=MAX_SENT_LEN)(sentence_input)\n",
    "\n",
    "l_LSTM = Bidirectional(LSTM(hidden_size,return_sequences = True))(embedded_sequences)\n",
    "l_dense = TimeDistributed(Dense(hidden_size))(l_LSTM)\n",
    "l_dense = Flatten()(l_dense)\n",
    "sentEncoder = Model(sentence_input,l_dense)\n",
    "\n",
    "body_input = Input(shape=(MAX_SENT_PER_ART,MAX_SENT_LEN,),dtype = 'int32')\n",
    "\n",
    "body_encoder = TimeDistributed(sentEncoder)(body_input)\n",
    "\n",
    "l_LSTM_sent = Bidirectional(LSTM(hidden_size,return_sequences=True))(body_encoder)\n",
    "l_dense_sent = TimeDistributed(Dense(hidden_size))(l_LSTM_sent)\n",
    "l_dense_sent = Flatten()(l_dense_sent)\n",
    "\n",
    "heading_input = Input(shape = (MAX_SENT_LEN, ),dtype = 'int32')\n",
    "heading_embedded_sequences = Embedding(output_dim=hidden_size, input_dim=vocab_size+1, \\\n",
    "                                       input_length = (MAX_SENT_LEN,), \\\n",
    "                                      weights = [embedding_matrix])(heading_input)\n",
    "h_dense = Dense(hidden_size,activation='relu')(heading_embedded_sequences)\n",
    "h_flatten = Flatten()(h_dense)\n",
    "article_output = concatenate([l_dense_sent,h_flatten],name = 'concatenate_heading')\n",
    "\n",
    "news_vestor = Dense(hidden_size,activation = 'relu')(article_output)\n",
    "preds = Dense(4,activation = 'softmax')(news_vestor)\n",
    "model = Model([body_input,heading_input],[preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "961c4571-36a5-492c-af9b-3880370db565",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b21d5f-2a72-4e29-bed1-02e6b8d2c72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "404/404 [==============================] - 759s 2s/step - loss: 0.6063 - categorical_accuracy: 0.7808 - val_loss: 0.7751 - val_categorical_accuracy: 0.6550\n",
      "Epoch 2/10\n",
      "134/404 [========>.....................] - ETA: 8:35 - loss: 0.3633 - categorical_accuracy: 0.8563"
     ]
    }
   ],
   "source": [
    "model.fit([X_train_body,X_train_head],[y_train], validation_data=([X_val_body,X_val_head],[y_val]), epochs=10 , batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8569bc-dc67-4c5b-b69c-2b1b59859ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"fnc-google300-embeddings\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
