{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1253dd1b-660d-4dc6-a83d-2c28fbd0aacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.dataset import DataSet\n",
    "from utils.generate_test_splits import generate_hold_out_split, read_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33c129e9-81ce-41c7-9b39-f32ddbd45702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset\n",
      "Total stances: 49972\n",
      "Total bodies: 1683\n"
     ]
    }
   ],
   "source": [
    "d = DataSet()\n",
    "generate_hold_out_split(d, training=0.8)\n",
    "trainID = set(read_ids(\"training_ids.txt\", \"splits\"))\n",
    "valID = set(read_ids(\"hold_out_ids.txt\", \"splits\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218d497e-a281-483e-919d-f7de19848b76",
   "metadata": {},
   "source": [
    "# Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fdf6f10-b426-42f6-b36d-18530b42f087",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_PER_ART = 5\n",
    "MAX_SENT_LEN = 20\n",
    "MAX_VOCAB = 50000\n",
    "VECTOR_SIZE = 100\n",
    "\n",
    "import gensim.downloader as api\n",
    "wv = api.load(f'glove-wiki-gigaword-{VECTOR_SIZE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde4bb2d-2708-47a7-973a-01e5fcc8a90d",
   "metadata": {},
   "source": [
    "# Set up training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b760b24-8e38-448d-907d-7af8f556427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stances = [stance for stance in d.stances if stance['Body ID'] in trainID]\n",
    "train_headlines = [stance['Headline'] for stance in train_stances]\n",
    "train_labels = [stance['Stance'] for stance in train_stances]\n",
    "train_body = [d.articles[stance['Body ID']] for stance in train_stances]\n",
    "\n",
    "val_stances = [stance for stance in d.stances if stance['Body ID'] in valID]\n",
    "val_headlines = [stance['Headline'] for stance in val_stances]\n",
    "val_labels = [stance['Stance'] for stance in val_stances]\n",
    "val_body = [d.articles[stance['Body ID']] for stance in val_stances]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65809091-a13b-4e7d-8a32-868df58d08fe",
   "metadata": {},
   "source": [
    "# Vectorization and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "181b27ca-c449-43e4-9590-f9adace47b60",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sw26wong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "nltk.download('punkt')\n",
    "\n",
    "vectorizer = TextVectorization(max_tokens=MAX_VOCAB, output_sequence_length=MAX_SENT_LEN)\n",
    "vectorizer.adapt(train_body + train_headlines + val_body + val_headlines)\n",
    "\n",
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "642b1bba-9a57-4a8f-a10a-f6ac30213805",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tok_art = []\n",
    "for article in train_body:\n",
    "    sent_tok_art.append(tokenize.sent_tokenize(article))\n",
    "\n",
    "vsent_tok_art = []\n",
    "for article in val_body:\n",
    "    vsent_tok_art.append(tokenize.sent_tokenize(article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cdf08ca-4e0d-44e9-a7d2-a03ef9c51cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_body = np.zeros((len(train_stances), MAX_SENT_PER_ART, MAX_SENT_LEN), dtype='int32')\n",
    "\n",
    "for i, article in enumerate(sent_tok_art):\n",
    "    for j, sentence in enumerate(article[:MAX_SENT_PER_ART]):\n",
    "        words = text_to_word_sequence(sentence)\n",
    "        for k, word in enumerate(words[:MAX_SENT_LEN]):\n",
    "            X_train_body[i][j][k] = word_index.get(word, 1) # get else UNK\n",
    "\n",
    "X_train_head = np.zeros((len(train_stances), MAX_SENT_LEN), dtype='int32')\n",
    "\n",
    "for i, headline in enumerate(train_headlines):\n",
    "    words = text_to_word_sequence(headline)\n",
    "    for j, word in enumerate(words[:MAX_SENT_LEN]):\n",
    "        X_train_head[i][j] = word_index.get(word, 1)\n",
    "\n",
    "X_val_body = np.zeros((len(val_stances), MAX_SENT_PER_ART, MAX_SENT_LEN), dtype='int32')\n",
    "\n",
    "for i, article in enumerate(vsent_tok_art):\n",
    "    for j, sentence in enumerate(article[:MAX_SENT_PER_ART]):\n",
    "        words = text_to_word_sequence(sentence)\n",
    "        for k, word in enumerate(words[:MAX_SENT_LEN]):\n",
    "            X_val_body[i][j][k] = word_index.get(word, 1)\n",
    "\n",
    "X_val_head = np.zeros((len(val_stances), MAX_SENT_LEN), dtype='int32')\n",
    "\n",
    "for i, headline in enumerate(val_headlines):\n",
    "    words = text_to_word_sequence(headline)\n",
    "    for j, word in enumerate(words[:MAX_SENT_LEN]):\n",
    "        X_val_head[i][j] = word_index.get(word, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfd06aba-72ed-49c9-98b0-7bac194968ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = pd.Series(train_labels)\n",
    "one_hot = pd.get_dummies(targets,sparse = True)\n",
    "one_hot_labels = np.asarray(one_hot)\n",
    "y_train = one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68403e35-626a-42d6-aa04-526e60dabd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = pd.Series(val_labels)\n",
    "one_hot = pd.get_dummies(targets,sparse = True)\n",
    "one_hot_labels = np.asarray(one_hot)\n",
    "y_val = one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8a72497-cdc8-4976-b67a-3fb3ef770c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['agree', 'disagree', 'discuss', 'unrelated']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a94ed6b-68b8-4b6e-8474-7fffd9e9a84a",
   "metadata": {},
   "source": [
    "# Create Embedding Matrix from Glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3323e80-750a-4051-be75-c556b5707f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_index)\n",
    "embedding_matrix = np.zeros((vocab_size+1, VECTOR_SIZE))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    try:\n",
    "        v = wv[word]\n",
    "        embedding_matrix[i] = v\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb1d8ea-0226-49f2-8994-abfd7114ea4a",
   "metadata": {},
   "source": [
    "# Reduce memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d0a0552-dbda-49d8-bd95-730048919fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del wv\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2260b21a-ab1e-4839-895d-220f5f01943d",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d731f70a-9fb0-4433-b205-05d03b7f74ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM, TimeDistributed, Activation\n",
    "from keras.layers import Flatten, Permute, merge, Input\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Input,Dense,multiply,concatenate,Dropout\n",
    "from keras.layers import GRU, Bidirectional\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5835d39-29c2-4ef0-8c71-cf6c1b01e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = VECTOR_SIZE\n",
    "trainable = True\n",
    "\n",
    "sentence_input = Input(shape=(MAX_SENT_LEN,),dtype='int32')\n",
    "sentence_embedding = Embedding(output_dim=hidden_size, \n",
    "                               input_dim=vocab_size+1, \n",
    "                               input_length=MAX_SENT_LEN,\n",
    "                               weights=[embedding_matrix],\n",
    "                               trainable=trainable,\n",
    "                               mask_zero=False,)(sentence_input)\n",
    "\n",
    "sentence_LSTM = Bidirectional(LSTM(hidden_size, return_sequences=True))(sentence_embedding)\n",
    "sentence_dense = TimeDistributed(Dense(hidden_size))(sentence_LSTM)\n",
    "sentence_dense = Flatten()(sentence_dense)\n",
    "sentence_encoder = Model(sentence_input,sentence_dense)\n",
    "\n",
    "body_input = Input(shape=(MAX_SENT_PER_ART,MAX_SENT_LEN,),dtype = 'int32')\n",
    "body_encoder = TimeDistributed(sentence_encoder)(body_input)\n",
    "body_LSTM = Bidirectional(LSTM(hidden_size,return_sequences=True))(body_encoder)\n",
    "body_dense = TimeDistributed(Dense(hidden_size))(body_LSTM)\n",
    "body_dense = Flatten()(body_dense)\n",
    "\n",
    "heading_input = Input(shape=(MAX_SENT_LEN,), dtype = 'int32')\n",
    "heading_embedding = Embedding(output_dim=hidden_size, input_dim=vocab_size+1, \n",
    "                                      input_length=MAX_SENT_LEN, \n",
    "                                      weights=[embedding_matrix],\n",
    "                                      trainable=trainable,\n",
    "                                      mask_zero=False,)(heading_input)\n",
    "heading_dense = Dense(hidden_size,activation='relu')(heading_embedding)\n",
    "heading_flatten = Flatten()(heading_dense)\n",
    "\n",
    "concatenated_input = concatenate([body_dense,heading_flatten],name='article')\n",
    "hidden_dense = Dense(hidden_size,activation='relu')(concatenated_input)\n",
    "prediction = Dense(4 ,activation='softmax')(hidden_dense)\n",
    "model = Model([body_input,heading_input],[prediction])\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f276971-9d06-4336-bb66-e040a56a8962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "316/316 [==============================] - 38s 105ms/step - loss: 0.6130 - categorical_accuracy: 0.7766 - val_loss: 0.6583 - val_categorical_accuracy: 0.7366\n",
      "Epoch 2/10\n",
      "316/316 [==============================] - 32s 102ms/step - loss: 0.3091 - categorical_accuracy: 0.8807 - val_loss: 0.6693 - val_categorical_accuracy: 0.7483\n",
      "Epoch 3/10\n",
      "316/316 [==============================] - 32s 102ms/step - loss: 0.1817 - categorical_accuracy: 0.9310 - val_loss: 0.5710 - val_categorical_accuracy: 0.7913\n",
      "Epoch 4/10\n",
      "316/316 [==============================] - 32s 102ms/step - loss: 0.1105 - categorical_accuracy: 0.9587 - val_loss: 0.5581 - val_categorical_accuracy: 0.8172\n",
      "Epoch 5/10\n",
      "316/316 [==============================] - 32s 102ms/step - loss: 0.0762 - categorical_accuracy: 0.9725 - val_loss: 0.6332 - val_categorical_accuracy: 0.8136\n",
      "Epoch 6/10\n",
      "316/316 [==============================] - 32s 102ms/step - loss: 0.0563 - categorical_accuracy: 0.9804 - val_loss: 0.6913 - val_categorical_accuracy: 0.8154\n",
      "Epoch 7/10\n",
      "316/316 [==============================] - 32s 102ms/step - loss: 0.0408 - categorical_accuracy: 0.9861 - val_loss: 0.7647 - val_categorical_accuracy: 0.8047\n",
      "Epoch 8/10\n",
      "316/316 [==============================] - 32s 101ms/step - loss: 0.0347 - categorical_accuracy: 0.9885 - val_loss: 0.8375 - val_categorical_accuracy: 0.8013\n",
      "Epoch 9/10\n",
      "316/316 [==============================] - 32s 101ms/step - loss: 0.0303 - categorical_accuracy: 0.9899 - val_loss: 0.8632 - val_categorical_accuracy: 0.8190\n",
      "Epoch 10/10\n",
      "316/316 [==============================] - 32s 101ms/step - loss: 0.0222 - categorical_accuracy: 0.9927 - val_loss: 1.0089 - val_categorical_accuracy: 0.7999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6df4479a90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X_train_body,X_train_head],[y_train], validation_data=([X_val_body,X_val_head],[y_val]), epochs=10 , batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49e95e5d-80f3-4d0a-9938-4267c3d4d78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset\n",
      "Total stances: 25413\n",
      "Total bodies: 904\n"
     ]
    }
   ],
   "source": [
    "cd = DataSet(\"competition_test\")\n",
    "\n",
    "\n",
    "test_stances = cd.stances\n",
    "test_headlines = [stance['Headline'] for stance in test_stances]\n",
    "test_labels = [stance['Stance'] for stance in test_stances]\n",
    "test_body = [cd.articles[stance['Body ID']] for stance in test_stances]\n",
    "\n",
    "X_test_body = np.zeros((len(cd.stances), MAX_SENT_PER_ART, MAX_SENT_LEN), dtype='int32')\n",
    "sent_tok_test = []\n",
    "for article in test_body:\n",
    "    sent_tok_test.append(tokenize.sent_tokenize(article))\n",
    "\n",
    "for i, article in enumerate(sent_tok_test):\n",
    "    for j, sentence in enumerate(article[:MAX_SENT_PER_ART]):\n",
    "        words = text_to_word_sequence(sentence)\n",
    "        for k, word in enumerate(words[:MAX_SENT_LEN]):\n",
    "            X_test_body[i][j][k] = word_index.get(word, 1)\n",
    "\n",
    "X_test_head = np.zeros((len(test_stances), MAX_SENT_LEN), dtype='int32')\n",
    "\n",
    "for i, headline in enumerate(test_headlines):\n",
    "    words = text_to_word_sequence(headline)\n",
    "    for j, word in enumerate(words[:MAX_SENT_LEN]):\n",
    "        X_test_head[i][j] = word_index.get(word, 1)\n",
    "\n",
    "\n",
    "predictions = model.predict([X_test_body,X_test_head])\n",
    "\n",
    "predicted_label = [LABELS[max([0, 1, 2, 3], key=lambda x: p[x])] for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d608ba8b-9517-4e5b-bfa3-3b44538b008c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7029866603706765"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = sum([pl == a for pl, a in zip(predicted_label, test_labels)])/len(test_labels)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3728af44-ce49-4777-b290-1ba997735a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(f\"glove{VECTOR_SIZE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
